# 02. 데이터 병합 및 샘플링


**예상 실행 시간 : 6시간**


**실행 가능한 소스 코드 파일 :** [02_Merge_and_Sampling.ipynb](02_Merge_and_Sampling.ipynb)


## 목표
데이터를 분석 가능한 형태로 만든다.
1. 사전 설정
2. 월별 데이터로 병합
3. 0값, 음수, 결측치 찾기
4. 데이터 샘플링


**NOTICE : 디스크 용량이 부족할 때**
```
- 데이터 병합 후 원본 csv 파일들 압축하거나 제거
- 데이터 샘플링 후 병합 csv 파일들 압축하거나 제거
```


**결과 파일**
```
sampled_(년월).csv - 45개 파일
sampled_(년도).csv - 4개 파일
```


## Step 1. 사전 설정
먼저 필요한 모듈을 import한다.
```python
import pandas as pd
import numpy as np
from scipy import stats
import gc # 메모리 부족 방지
import os 
import math 
import warnings
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.font_manager as fm

warnings.filterwarnings("ignore")
```
파일 위치 전역 변수를 설정한다.
```python
DATA_RAW_DIR = './data/raw'
DATA_OUTPUT_DIR = './data/processed'
ANALYSIS_OUTPUT_DIR = './outputs/tables'
SAMPLING_OUTPUT_DIR = './data/external'
```
한글 표시를 위한 한글 폰트를 설정한다.
```python
fe = fm.FontEntry(
    fname=r'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', 
    name='NanumGothic')
fm.fontManager.ttflist.insert(0, fe)
plt.rcParams.update({'font.size': 18, 'font.family': 'NanumGothic'})
mpl.rcParams['axes.unicode_minus'] = False
```

## Step 2. 월별 데이터로 병합
원본 데이터 파일 디렉토리(./data/raw)에서 csv 파일들을 불러온다.
```python
# 파일의 압축을 푼 뒤 실행한다.
all_files = os.listdir(DATA_RAW_DIR)

csv_files = sorted([f for f in all_files if f.startswith('tbsh_gyeonggi_day_') 
    and f.endswith('.csv')])
```
병합 데이터 파일 디렉토리(./data/processed)에서 csv파일이 있는지 찾는다.
```python
exist_files = sorted([f for f in os.listdir(DATA_OUTPUT_DIR) if f.startswith('gyeonggi_')
    and f.endswith('.csv')])
```
exist_files가 빈 리스트일 때는 target_months가 모든 파일이 된다.
```python
# 이미 작업한 개월 
aleady_processed_months = {f.replace('gyeonggi_', '').replace('.csv', '') 
    for f in exist_files}

# 범위 설정
all_months = [x for x in [f"{year}{month:02d}" for year in range(2022, 2026) 
    for month in range(1, 13)] if x <= "202509"]

# 작업 해야하는 개월
target_months = sorted([x for x in all_months if x not in aleady_processed_months])
```
병합을 위해 파일을 월별로 그룹화 시켜놓는다.
```python
# 파일명에서 연월 추출하는 함수
def extract_month(filename):
    parts = filename.split('_')
    if len(parts) >= 4:
        return parts[3]
    return None

monthly_files = {}

for filename in csv_files:
    months = extract_month(filename)
    if months:
        monthly_files.setdefault(months, []).append(filename)
```
월별로 csv 파일을 병합한다.
```python
for month in sorted(target_months):
    month_files = monthly_files[month]
    dfs = []
    
    # 인코딩 관련 에러 해결 코드 추가
    for filename in month_files:
        filepath = os.path.join(DATA_RAW_DIR, filename)
        df_temp = None
        for encoding in ['utf-8', 'euc-kr', 'cp949']:
            try:
                df_temp = pd.read_csv(filepath, encoding=encoding)
                break
            except:
                continue
        if df_temp is not None:
            dfs.append(df_temp)
    
    if len(dfs) > 0:
        df_month = pd.concat(dfs, ignore_index=True)
        
        # 저장 경로
        output_file = os.path.join(DATA_OUTPUT_DIR, f"gyeonggi_{month}.csv")
        
        # csv로 저장
        df_month.to_csv(output_file, index=False)
        
        success_count += 1
        
        # 메모리 정리 코드 추가
        del df_month, dfs
        gc.collect()
```
### 에러 해결 과정

**파일의 인코딩이 월별로 달라 에러가 발생해서 아래와 같은 코드를 작성했다.**
```python
import pandas as pd

for encoding in ['utf-8', 'euc-kr', 'cp949']:
    try:
        df_temp = pd.read_csv(filepath, encoding=encoding)
        break
    except:
        continue
    if df_temp is not None:
        dfs.append(df_temp)
```
**파일의 크기가 너무 크기 때문에 작업 중에 메모리가 부족해져서 아래와 같은 코드를 추가했다.**
```python
import gc

del df_month, dfs
gc.collect()
```

## Step 3. 0값, 음수, 결측치 찾기
숫자 데이터에서 0 값을 찾는 함수를 정의한다.
```python
def df_zero(df, columns=None):
    zeros = {}
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns
    for col in columns:
        count = (df[col] == 0).sum()
        if count > 0:
            zeros[col] = {
                'count' : count,
                'percentage' : (count/len(df))*100
            }
    
    return zeros
```
숫자 데이터에서 음수 값을 찾는 함수를 정의한다.
```python
def df_negative(df):
    negatives = {}
    for col in df.select_dtypes(include=[np.number]).columns:
        count = (df[col]<0).sum()
        if count > 0:
            negatives[col] = {
                'count': count,
                'percentage': (count/len(df))*100,
                'min_value': df[col].min()
            }

    return negatives
```
병합 파일에서 결측치 및 이상치를 찾는다.
```python
for filename in processed_files:
    month = filename.replace('gyeonggi_', '').replace('.csv', '')
    
    try:
        filepath = os.path.join(DATA_OUTPUT_DIR, filename)       
        df = pd.read_csv(filepath)
        
        missing_count = df.isnull().sum().sum()
        missing_by_col = {col: {'count': df[col].isnull().sum(), 
                                'percentage': round((df[col].isnull().sum() 
                                    / len(df)) * 100, 4)}
                            for col in df.columns if df[col].isnull().sum() > 0}
        
        duplicate_count = df.duplicated().sum()
        negatives = df_negative(df)
        zeros = df_zero(df)
        
        search_report = {
            'month': month,
            'data_shape': {'rows': len(df), 'columns': len(df.columns)},
            'results': {
                'missing_values': {'total_missing': missing_count, 
                                   'by_column': missing_by_col},
                'duplicates': {'total_duplicates': duplicate_count, 
                               'percentage': round((duplicate_count / len(df)) * 100, 4)},
                'negative_values': negatives,
                'zero_values': zeros
            }
        }
        
        search_results.append(search_report)
        
        del df
        gc.collect()
    
    except Exception as e:
        print(f"\n X {month} 오류: {str(e)[:40]}")
```
결과를 csv파일로 저장한다.
```python
summary_data = []
for result in search_results:
    missing = result['results']['missing_values']
    duplicate = result['results']['duplicates']
    negative = result['results']['negative_values']
    zero = result['results']['zero_values']

    summary_data.append({
        'month': result['month'],
        'total_rows': result['data_shape']['rows'],
        'missing_count': missing['total_missing'],
        'duplicate_count': duplicate['total_duplicates'],
        'negative_count': sum(v['count'] for v in negative.values()) if negative else 0,
        'zero_count': sum(v['count'] for v in zero.values()) if zero else 0
    })

summary_df = pd.DataFrame(summary_data)
csv_output = os.path.join(ANALYSIS_OUTPUT_DIR, 'searching_summary.csv')
summary_df.to_csv(csv_output, index=False, encoding='utf-8-sig')
```
결과를 확인한다.
```python
summary_df = pd.read_csv(os.path.join(ANALYSIS_OUTPUT_DIR, 'searching_summary.csv'))
summary_df.head()
```

|   | month  | total_rows | missing_count | duplicate_count | negative_count | zero_count |
|---|-------:|-----------:|--------------:|----------------:|---------------:|-----------:|
| 0 | 202201 | 11479842   | 0             | 0               | 0              | 89         |
| 1 | 202202 | 10191338   | 0             | 0               | 0              | 76         |
| 2 | 202203 | 12106986   | 0             | 0               | 0              | 115        |
| 3 | 202204 | 12500585   | 0             | 0               | 0              | 148        |
| 4 | 202205 | 13218383   | 0             | 0               | 0              | 153        |

해당 결과는 0값만 제거해도 된다는 사실을 시사한다.
따라서 아래와 같은 코드로 0값을 제거한다.
```python
for month in drop_target_month:
    file_path = os.path.join(DATA_OUTPUT_DIR, f'gyeonggi_{month}.csv')
    
    try:
        df = pd.read_csv(file_path)
        rows_before = len(df)
        
        for col in df.select_dtypes(include=[np.number]).columns:
            df = df[df[col] != 0]
            
        rows_after = len(df)
        removed_rows = rows_before - rows_after

        df.to_csv(file_path, index=False, encoding='utf-8-sig')
            
        processed_count += 1
            
        del df
        gc.collect()
```

## Step 4. 데이터 샘플링
**이 단계에서는 아래 링크의 문서를 참고했습니다.**

1 : [샘플 크기 결정](https://en.wikipedia.org/wiki/Sample_size_determination)

2 : [유한 수정 계수](https://hs-mathematics.tistory.com/)

3 : [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html)

4 : [데이터 분할](https://wikidocs.net/145289)

데이터 특성상 샘플링 비율이 5% 이하이면 특정 카테고리는 제외될 가능성이 있다고 판단해서 5%로 정했다.

지금부터는 샘플링 비율을 5%로 정하는 것이 타당한지 알아보겠다.

표준 점수는 신뢰도에서 평균을 뺀 값을 표준편차로 나눈 값이다.

<img src="https://latex.codecogs.com/svg.image?\textit{z}_{\alpha/2}=\phi^{-1}\left(1-\frac{\alpha}{2}\right)">

scipy을 사용해서 아래와 같은 방식으로 구할 수 있다.
```python
z_score = stats.norm.ppf((1 - confidence_level) / 2)
```
모집단의 크기가 무한이라고 가정하면 샘플의 크기는 다음과 같다.

<img src="https://latex.codecogs.com/svg.image?\textit{n}_{0}=\frac{\textit{z}_{\alpha/2}^{2}\cdot\textit{p}\cdot\left(1-\textit{p}\right)}{\textit{e}^{2}}">

그러나 모집단의 크기가 매우 커도 무한이 아니기 때문에 유한 수정계수를 적용해야 한다.

<img src="https://latex.codecogs.com/svg.image?\textit{n}=\left\lceil\frac{\textit{n}_{0}}{1&plus;\frac{\textit{n}_{0}-1}{\textit{N}}}\right\rceil">

다음과 같은 코드로 샘플링 비율을 구할 수 있다.
```python
n = (z_score**2 * p * (1-p)) / (margin_error**2)
min_sample_size = math.ceil(n / (1 + ((n - 1) / population_size)))
sampling_ratio = (n / population_size) * 100
```
5% > 0.003% 이므로 샘플 크기를 5%로 잡아도 통계적 왜곡이 없다는 사실을 알 수 있다.
```
월별 csv 파일 raw 수 평균: 12835278
최소 표본 크기: 385
권장 샘플링 비율: 0.003%
```
샘플링을 위해 아래와 같은 코드를 작성한다.
처음에는 이러한 함수를 정의했다.
```python
dtypes = {
    'card_tpbuz_cd': 'category',# 카드사 업종
    'admi_cty_no': 'int32',     # 지역구
    'ta_ymd': 'int32',          # 년월일
    'hour': 'int8',             # 시간대 
    'day': 'int8',              # 요일
    'age': 'int8',              # 연령 
    'sex': 'category',          # 성별
    'amt': 'int32',             # (전수화 보정이 적용된) 매출금액
    'cnt': 'int16'              # (전수화 보정이 적용된) 매출건수
}

def stratified_sampling(file_path, target_ratio=0.05):
    df = pd.read_csv(file_path, dtype=dtypes)

    stratify = ['card_tpbuz_cd', 'admi_cty_no', 'ta_ymd']
    stratify = [col for col in stratify if col in df.columns]

    df['_strata'] = df[stratify].apply(lambda x: tuple(x), axis=1)
    
    sampled_indices = []

    for strata_value, group_df in df_copy.groupby('_strata', sort=False):
        group_size = len(group_df)
        sample_size = max(1, int(group_size * target_ratio))
        
        sample_idx = np.random.choice(group_df.index, size=sample_size, replace=False)
        sampled_indices.extend(sample_idx)
    
    df_sampled = df.loc[sampled_indices].reset_index(drop=True)
    df_sampled = df_sampled.drop(columns=['_strata'])
    df.drop(columns=['_strata'], inplace=True)
    
    return df_sampled
```
그러나 45개중 1개 개월만 테스트했을 때 시간이 오래 걸리는 문제가 발생했다. (약 4시간 정도 걸릴 것으로 예상됨.)

그래서 기계학습, 모델링 등에 사용되는 scikit-learn의 model_selection.train_test_split 클래스가 데이터를 학습과 검증 세트로 나누어 준다는 점이 일종의 샘플링이 아닌가 하는 아이디어에서 착안하여 이를 적용하여 아래와 같은 함수를 정의했다.
```python
# 계층화 샘플링 함수 정의

from sklearn.model_selection import train_test_split

# 샘플링 할 데이터
dtypes = {
    'card_tpbuz_cd': 'category',# 카드사 업종
    'admi_cty_no': 'int32',     # 지역구
    'ta_ymd': 'int32',          # 년월일
    'hour': 'int8',             # 시간대 
    'day': 'int8',              # 요일
    'age': 'int8',              # 연령 
    'sex': 'category',          # 성별
    'amt': 'int32',             # (전수화 보정이 적용된) 매출금액
    'cnt': 'int16'              # (전수화 보정이 적용된) 매출건수
}

# 단 한 개만 존재하는 그룹이 있으면 기존 방식 사용
def stratified_sampling(df, target_ratio=0.05, random_state=42):

    stratify = ['card_tpbuz_cd', 'admi_cty_no', 'ta_ymd']

    multi = df.duplicated(subset=stratify, keep=False)
    df_multi_groups = df[multi]
    df_single_groups = df[~multi]

    df_sampled = []

    if len(df_multi_groups) > 0 :
        indices = np.arange(len(df))
        try:
            sampled_idx, _ = train_test_split(
                indices, test_size=1 - target_ratio, random_state=random_state,
                stratify=df_multi_groups[stratify].values
            )
            df_sampled.append(df_multi_groups.iloc[sampled_idx])
        except:
            df_sampled.append(df_multi_groups.sample(frac=target_ratio, random_state=random_state))

    if len(df_single_groups) > 0:
        dsg_sample = df_single_groups.sample(frac=target_ratio, random_state=random_state)
        df_sampled.append(dsg_sample)

    if df_sampled:
        return pd.concat(df_sampled, ignore_index=True)
    else:
        return pd.DataFrame(columns=df.columns)

```
아래의 코드로 모든 파일에 대해 샘플링을 실시한다.
```python
# 모든 월 일괄 샘플링
processed_files = sorted([f for f in os.listdir(DATA_OUTPUT_DIR) 
    if f.startswith('gyeonggi_') and f.endswith('.csv')])

for filename in processed_files:
    try:
        file_path = os.path.join(DATA_OUTPUT_DIR, filename)

        # 파일 읽기
        df = pd.read_csv(file_path, dtype=dtypes)
        
        # 샘플링
        df_sampled = stratified_sampling(df, target_ratio=0.05)
        
        # 저장
        output_file = os.path.join(SAMPLING_OUTPUT_DIR, f'sampled_{month}.csv')
        df_sampled.to_csv(output_file, index=False, encoding='utf-8-sig')

        del df, df_sampled
        gc.collect()
```
이전 코드보다 약 5~6배정도 빠를 것이다. (실제 실행 시간 30분)

그 후 연도별 파일로 병합한 파일을 생성한다.
```python
# 샘플 파일을 연도 별로 병합

# 1. 파일 목록 불러오기
sampled_files = sorted([f for f in os.listdir(SAMPLING_OUTPUT_DIR) if f.startswith('sampled_') and f.endswith('.csv')])

files_by_year = {}

for filename in sampled_files:
    month_str = filename.replace('sampled_', '').replace('.csv', '')
    years = month_str[:4]
    files_by_year.setdefault(years, []).append(filename)

# 2. 병합
for year in sorted(files_by_year.keys()):
    merged_dfs = []

    for filename in sorted(files_by_year[year]):       
        file_path = os.path.join(SAMPLING_OUTPUT_DIR, filename)
        
        month_str = filename.replace('sampled_', '').replace('.csv', '')

        df = pd.read_csv(file_path)

        merged_dfs.append(df)

        gc.collect()

    df_year = pd.concat(merged_dfs, ignore_index=True)

    output_file = os.path.join(SAMPLING_OUTPUT_DIR, f'sampled_{year}.csv')

    df_year.to_csv(output_file, index=False, encoding='utf-8-sig')

    del df_year, merged_dfs
    gc.collect()
```